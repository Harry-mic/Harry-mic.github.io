---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a third year M.S. student at SIGS, Tsinghua University and I'm very fortune to be advised by Prof. Xueqian Wang. Before that, I received my bachelor’s degree in Electrical Engineering and Automation from Xi’an Jiaotong University in Jun. 2022. My currect research interest lies in RL and LLM, especially in alignment with AI feedback and AI safety. 

Currently, I'm an intern at Tencent AI Lab, AI for Science Center, supervised by Peilin Zhao.



Publications & Preprints
======

<table style="border-collapse: collapse; margin: 0; border: none;">
    <tr>
    <td style="border: none;"><img src="/images/WechatIMG476.png" alt="Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation" width="600"></td>
    <td style="border: none;"><a href="https://arxiv.org/abs/2408.10668"> Probing the Safety Response Boundary of Large Language Models via<br> Unsafe Decoding Path Generation</a>  <br><strong>Haoyu Wang</strong>, Bingzhe Wu, Yatao Bian, Yongzhe Chang, Xueqian Wang, Peilin Zhao<br> <span style="font-size: 0.8em;">Under Review </span> </td>
  </tr>
    <tr>
    <td style="border: none;"><img src="/images/WechatIMG475.png" alt="Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping" width="600"></td>
    <td style="border: none;"> <a href="https://arxiv.org/abs/2402.07610"> Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping </a> <br>  <strong>Haoyu Wang</strong>, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao <br> <span style="font-size: 0.8em;">MHFAIA Workshop at ICML 2024 & Under Review</span>   </td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/images/WechatIMG474.png" alt="Are Large Language Models Really Robust to Word-Level Perturbations?" width="600"></td>
    <td style="border: none;"><a href="https://arxiv.org/abs/2309.11166"> Are Large Language Models Really Robust to Word-Level Perturbations?</a> <br> <strong>Haoyu Wang</strong>, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao <br> <span style="font-size: 0.8em;">SoLaR Workshop at NeurIPS 2023 & Under Review</span></td>
  </tr>
  <tr>
    <td style="border: none;"><img src="/images/nips2023.png" alt="Learning better with less: effective augmentation for sample-efficient visual reinforcement learning" width="600"></td>
    <td style="border: none;"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/bc26087d3f82e62044fc77752e86737e-Abstract-Conference.html"> Learning better with less: effective augmentation for sample-efficient<br> visual reinforcement learning</a> <br>Guozheng Ma, Linrui Zhang, <strong>Haoyu Wang</strong>,  Lu Li, Zilin Wang, Zhen Wang, Li Shen, Xueqian Wang, Dacheng Tao <br><span style="font-size: 0.8em;">Advances in Neural Information Processing Systems 2023 </span> </td>
  </tr>
</table>

Education
======
·2022.09 - Now, Tsinghua University, Master, Big Data Program

·2018.09 - 2022.06, Xi'an Jiaotong University, Bachelor, Electrical Engineering and Automation 

Internship
======

·2023.09 - Now, Tencent AI Lab, AI for Science Center, supervised by Peilin Zhao
